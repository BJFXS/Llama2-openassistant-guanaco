# -*- coding: utf-8 -*-
"""llama_2_openassistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bxmDUeXIhzo4FhAPInZK0uJQ8uCEne4P

Based on
*   https://huggingface.co/blog/dpo-trl
*   https://github.com/huggingface/trl/blob/main/examples/

### **Preparations:**

**Download libraries**: such as transformers, accelerate, peft, datasets and TRL
"""

!pip install -q huggingface_hub
!pip install -q -U trl transformers accelerate peft
!pip install -q -U datasets bitsandbytes einops wandb
!pip install  -q ipywidgets
!pip install -q scipy

"""**Import modules**: such as os, torch and trl"""

import os
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset
from peft import AutoPeftModelForCausalLM, LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer
from trl.trainer import ConstantLengthDataset

"""**login huggingface**:"""

from huggingface_hub import notebook_login
notebook_login()

"""### **Define the required variables and functions:**"""

model_name = "TinyPixel/Llama-2-7B-bf16-sharded" ##"meta-llama/Llama-2-7b-hf"
log_with = "wandb"
dataset_name = "timdettmers/openassistant-guanaco" #"lvwerra/stack-exchange-paired"
subset = "data/finetune" # not in use
split = "train"

size_valid_set = 4000
streaming = True
shuffle_buffer = 5000
seq_length = 1024
max_steps = -1
logging_steps = 5
save_steps = 10
per_device_train_batch_size = 4
per_device_eval_batch_size = 1
gradient_accumulation_steps = 2

gradient_checkpointing = True
num_train_epochs=3

group_by_length = True
lora_alpha = 16
lora_dropout = 0.05
lora_r = 8

learning_rate = 2e-4 #
lr_scheduler_type = "cosine"
num_warmup_steps = 100
weight_decay = 0.05
optimizer_type = "paged_adamw_32bit"
##################

output_dir = "./results"
log_freq = 10

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

def prepare_sample_text(example):
    """Prepare the text from a sample of the dataset."""
    text = example['text'] ##f"Question: {example['question']}\n\nAnswer: {example['response_j']}"
    return text

#########  Estimate the average number of characters per token in the dataset. #########
def chars_token_ratio(dataset, tokenizer, nb_examples=400):
    total_characters, total_tokens = 0, 0
    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):
        text = prepare_sample_text(example)
        total_characters += len(text)
        if tokenizer.is_fast:
            total_tokens += len(tokenizer(text).tokens())
        else:
            total_tokens += len(tokenizer.tokenize(text))

    return total_characters / total_tokens

def create_datasets(tokenizer):
  train_dataset = load_dataset( dataset_name, split='train')
  train_data = train_dataset ##.select(range(70))
  print("training sample:")
  print(train_data['text'][0])
  eval_dataset = load_dataset( dataset_name, split='test')
  valid_data = eval_dataset ##.select(range(100))
  print("validation sample:")
  print(valid_data['text'][1])

  chars_per_token = chars_token_ratio(train_data, tokenizer)
  print(f"The character to token ratio of the dataset is: {chars_per_token:.2f}")

  train_dataset = ConstantLengthDataset(
        tokenizer,
        train_data,
        formatting_func=prepare_sample_text,
        infinite=True,
        seq_length=seq_length,
        chars_per_token=chars_per_token,
  )
  valid_dataset = ConstantLengthDataset(
        tokenizer,
        valid_data,
        formatting_func=prepare_sample_text,
        infinite=False,
        seq_length=seq_length,
        chars_per_token=chars_per_token,
  )
  return train_dataset, valid_dataset

"""### **Download the model and tokenizer**:"""

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map={"": 0},           # 无 #
    trust_remote_code=True,
)
base_model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"  # Fix weird overflow issue with fp16 training

"""**Set parameters**:"""

peft_config = LoraConfig(
    r=lora_r,
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    target_modules=["q_proj", "v_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)

num_train_epochs=2

training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    per_device_eval_batch_size=per_device_eval_batch_size,
    learning_rate=learning_rate,
    logging_steps=logging_steps,
    max_steps=max_steps,
    report_to=log_with,
    save_steps=save_steps,
    #group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    warmup_steps=num_warmup_steps,
    optim=optimizer_type,
    bf16=True,
    #bf16=False,
    remove_unused_columns=False,
    run_name="sft_llama2",
    num_train_epochs=num_train_epochs,
)

train_dataset, eval_dataset = create_datasets(tokenizer)

"""***Test the output before training***"""

mystring = "### Human: Can you write a short introduction about the relevance of the term 'monopsony' in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant:"
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = """### Human: Where did fortune cookies originate? ### Assistant:"""
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring ="### Human: What is the dark ring around the iris of my eye? ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring ="### Human: Is it normal to have a dark ring around the iris of my eye? ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: give the definition of contrastive learning in machine learning. ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring ="### Human: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?  ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: How to mount the docker socket when I using docker compose?  ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: I am using docker compose and i need to mount the docker socket - how would i do that?  ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: Give some cool activities for a dungeons and dragons campaign that players can do when they reach the hellbound city of Dis. ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: How does a Raspberry Pi differ from an ESP32? Which one is more suitable for interfacing with an SD card? ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: What is the difference between a raspberry pi and an esp32? What is better suited for interfacing with a SD card?	 ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = """### Human: Can you explain to me how the stable diffusion algorithm for ai generated images can be used to enhance the visual quality of low resolution and low quality images? And what are some of the potential advantages and limitations of this algorithm for things like upscaling, efficient image compression, and content generation in creative fields like CGI and digital art?### Assistant:"""
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

"""### **Start Training**:"""

############# 训练 ############
trainer = SFTTrainer(
    model=base_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    packing=True,
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_args,
)
trainer.train()
trainer.save_model(output_dir)

output_dir = os.path.join(output_dir, "final_checkpoint")
trainer.model.save_pretrained(output_dir)

"""**upload merged model to huggingface：**"""

from huggingface_hub import notebook_login
notebook_login()

del base_model
torch.cuda.empty_cache()
##output_dir = output_dir ##'outputs'##"/content/results/checkpoint-10"
merged_model = AutoPeftModelForCausalLM.from_pretrained(output_dir,  torch_dtype=torch.bfloat16) #device_map="auto",
merged_model = merged_model.merge_and_unload()

output_merged_dir = os.path.join(output_dir, "final_merged_checkpoint")
merged_model.save_pretrained(output_merged_dir, safe_serialization=True)

new_model_name = "llama2-7b-finetunned-openassistant-origin"

merged_model.push_to_hub(new_model_name, max_shard_size = "2GB") ######################

"""～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～

### **Test the output after training**:
"""

new_model_name = "llama2-7b-finetunned-openassistant-origin"

"""**download the trained model from huggingface**"""

#######从我的HF (huggingface)账号读取刚才存的训练结果######

import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments
import torch

model_name = new_model_name ##"llama2-7b-finetunned-openassistant-merged_test"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    #bnb_4bit_use_double_quant=True,    #####无用#####
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

base_model = AutoModelForCausalLM.from_pretrained(
    #"ScottShao/" + model_name,
    "bjfxs/" + model_name,
    # output_merged_dir,
    quantization_config=bnb_config,
    device_map={"": 0},
    trust_remote_code=True,
    #use_auth_token=True,
)

base_model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained("TinyPixel/Llama-2-7B-bf16-sharded")

mystring = "### Human: Can you write a short introduction about the relevance of the term 'monopsony' in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant:"
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=400, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = """### Human: Where did fortune cookies originate? ### Assistant:"""
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring ="### Human: What is the dark ring around the iris of my eye? ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=200, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = """### Human: Is it normal to have a dark ring around the iris of my eye?### Assistant:"""
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=400, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: give the definition of contrastive learning in machine learning. ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=400, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = """### Human: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?### Assistant:"""
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=700, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: How to mount the docker socket when I using docker compose?  ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=600, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: I am using docker compose and i need to mount the docker socket - how would i do that?  ### Assistant:"

encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=400, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: Give some cool activities for a dungeons and dragons campaign that players can do when they reach the hellbound city of Dis. ### Assistant:"
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=400, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: How does a Raspberry Pi differ from an ESP32? Which one is more suitable for interfacing with an SD card? ### Assistant:"
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=400, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = "### Human: What is the difference between a raspberry pi and an esp32? What is better suited for interfacing with a SD card?	 ### Assistant:"
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=400, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))

mystring = """### Human: Can you explain to me how the stable diffusion algorithm for ai generated images can be used to enhance the visual quality of low resolution and low quality images? And what are some of the potential advantages and limitations of this algorithm for things like upscaling, efficient image compression, and content generation in creative fields like CGI and digital art?### Assistant:"""
encoding = tokenizer(mystring, return_tensors="pt").to("cuda:0")
output = base_model.generate(input_ids=encoding.input_ids, attention_mask=encoding.attention_mask, max_new_tokens=400, do_sample=True, temperature=0.000001, eos_token_id=tokenizer.eos_token_id, top_k = 0)

print("Question:")
print(mystring)
print("Answer:")
print(tokenizer.decode(output[0], skip_special_tokens=True))