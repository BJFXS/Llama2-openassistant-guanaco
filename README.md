# Llama2-openassistant-guanaco
This is a fine-tuning model project aimed at enhancing the text generation capabilities of a simplified version of the Llama-2 model. Llama-2 is a powerful pre-trained language model, and the objective of fine-tuning this simplified version is to improve its performance on text generation tasks. To achieve this goal, I used a diverse dataset consisting of multiple languages and domains for training. By fine-tuning on such a diverse dataset, my aim is to enable the simplified Llama-2 model to generate high-quality text with greater accuracy and fluency, regardless of the language or domain.


# Based on: 
https://huggingface.co/blog/dpo-trl

https://github.com/huggingface/trl/blob/main/examples/
